import os
import time  # <--- INDISPENSABLE pour attendre que l'audio soit prÃªt
import google.generativeai as genai
from dotenv import load_dotenv

load_dotenv()

class Brain:
    def __init__(self):
        # Configuration de l'API
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            print("ðŸ”´ ERREUR : Pas de GEMINI_API_KEY dans .env")
        
        genai.configure(api_key=api_key)

        # ModÃ¨le : On utilise Flash 1.5 (Rapide + Multimodal Audio)
        self.model_name = "gemini-1.5-flash"
        
        # Variables d'Ã©tat
        self.chat = None
        self.system_instruction = "Tu es un assistant utile."
        
        # Initialisation par dÃ©faut
        self.init_model()

    def init_model(self):
        """Initialise ou RÃ©initialise le modÃ¨le avec l'instruction actuelle"""
        try:
            self.model = genai.GenerativeModel(
                model_name=self.model_name, 
                system_instruction=self.system_instruction
            )
            # On dÃ©marre une nouvelle session de chat (historique vide)
            self.chat = self.model.start_chat(history=[])
            print(f"ðŸ§  Cerveau prÃªt : {self.model_name}")
        except Exception as e:
            print(f"ðŸ”´ Erreur chargement modÃ¨le : {e}")

    def update_persona(self, name, scenario, behavior):
        """Met Ã  jour l'identitÃ© de l'IA et reset la conversation"""
        self.system_instruction = f"""
        Tu incarnes {name}.
        SCÃ‰NARIO : {scenario}
        COMPORTEMENT : {behavior}
        
        RÃˆGLES IMPORTANTES :
        - Tu es dans une conversation ORALE (Audio).
        - Tes rÃ©ponses seront lues par une voix TTS.
        - Ne fais JAMAIS de listes Ã  puces, de markdown (gras/italique) ou d'Ã©mojis complexes.
        - Fais des phrases courtes, naturelles et percutantes.
        - Si l'utilisateur hÃ©site, encourage-le.
        """
        print(f"ðŸ§  Persona mise Ã  jour : {name}")
        # On recharge le modÃ¨le pour appliquer la nouvelle instruction systÃ¨me
        self.init_model()

    def clear_history(self):
        """Efface la mÃ©moire de la conversation (Pour le bouton STOP)"""
        print("ðŸ§¹ Nettoyage de l'historique...")
        self.init_model() # Le fait de rÃ©init le modÃ¨le vide l'historique

    def think_from_audio(self, audio_path):
        """
        ReÃ§oit un fichier audio, l'envoie Ã  Gemini, ATTEND qu'il soit prÃªt,
        et retourne la rÃ©ponse textuelle.
        """
        try:
            print(f"ðŸ‘‚ Brain Ã©coute le fichier : {audio_path}")
            
            # 1. Upload du fichier vers Google
            audio_file = genai.upload_file(path=audio_path)
            
            # 2. ATTENTE ACTIVE (Correction Erreur 400)
            # Gemini a besoin de 1-2 secondes pour traiter l'audio avant de pouvoir l'utiliser
            print("â³ Attente du traitement audio par Google...")
            while audio_file.state.name == "PROCESSING":
                time.sleep(1)
                audio_file = genai.get_file(audio_file.name)
            
            if audio_file.state.name != "ACTIVE":
                raise Exception(f"Fichier audio refusÃ© par Google : {audio_file.state.name}")

            # 3. Envoi dans le CHAT (pour garder la mÃ©moire de la conversation)
            # On envoie juste le fichier, l'instruction systÃ¨me est dÃ©jÃ  chargÃ©e dans self.chat
            response = self.chat.send_message([audio_file])
            
            text_response = response.text
            print(f"ðŸ§  RÃ©ponse gÃ©nÃ©rÃ©e : {text_response[:50]}...")
            
            return text_response

        except Exception as e:
            print(f"ðŸ”´ Erreur Brain Audio : {e}")
            return "DÃ©solÃ©, je n'ai pas bien entendu ce que tu as dit."

    def think_streaming(self, user_text):
        """Fonction de secours pour le chat textuel classique"""
        if not self.chat: return
        try:
            response = self.chat.send_message(user_text, stream=True)
            for chunk in response:
                if chunk.text:
                    yield chunk.text
        except Exception as e:
            print(f"ðŸ”´ Erreur Chat Texte : {e}")
            yield "Erreur technique."

    def analyze_pitch(self, prompt_context):
        """
        Analyse le pitch (Appel unique, pas besoin de mÃ©moire de chat ici).
        On utilise generate_content directement.
        """
        print(f"ðŸ“Š Analyse Pitch en cours...")
        
        analysis_instruction = """
        Tu es un expert en Pitch de Startup (Type Y-Combinator).
        Analyse ce pitch. Sois SÃ‰VÃˆRE mais JUSTE.
        
        RÃ©ponds UNIQUEMENT avec ce JSON strict (sans Markdown) :
        {
            "note": "Note sur 100 (ex: 65/100)",
            "accroche_probleme": "Analyse du WHY/ProblÃ¨me (1 phrase)",
            "solution_cible": "Analyse du Solution/Cible (1 phrase)",
            "unicite_business": "Analyse DiffÃ©renciation/Business (1 phrase)",
            "cta_action": "Analyse du Call to Action (1 phrase)",
            "elements_manquants": "Liste les Ã©lÃ©ments cruciaux oubliÃ©s (ou 'Aucun' si complet)",
            "conseil": "TON meilleur conseil pour amÃ©liorer ce pitch"
        }
        """
        
        try:
            # On utilise le modÃ¨le sans historique pour une analyse one-shot
            analysis_model = genai.GenerativeModel(
                model_name=self.model_name,
                system_instruction=analysis_instruction
            )
            
            response = analysis_model.generate_content(prompt_context)
            
            # Nettoyage du Markdown json si Gemini en met
            clean_json = response.text.replace("```json", "").replace("```", "").strip()
            return clean_json
            
        except Exception as e:
            print(f"ðŸ”´ CRASH ANALYSE : {e}")
            # JSON de secours pour Ã©viter que le frontend plante
            return '{"note": "0/100", "accroche_probleme": "Erreur", "solution_cible": "Erreur", "unicite_business": "Erreur", "cta_action": "Erreur", "elements_manquants": "Erreur technique IA", "conseil": "VÃ©rifiez la connexion API."}'